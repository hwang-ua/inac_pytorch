import os
import torch

from core.agent import base

"""
https://github.com/rlai-lab/Regularized-GradientTD/blob/master/TDRC/DQRC.py
"""
class QRCOnline(base.ValueBased):
    # def __init__(self, features, actions, policy_net, target_net, optimizer, params, device=None):
    def __init__(self, cfg):
        super(QRCOnline, self).__init__(cfg)
        self.features = self.rep_net.output_dim
        self.actions = cfg.action_dim

        # regularization parameter
        self.alpha = cfg.learning_rate
        self.beta = cfg.qrc_beta

        # secondary weights optimization parameters
        self.beta_1 = cfg.qrc_beta_1 #0.99
        self.beta_2 = cfg.qrc_beta_2 #0.999

        # learnable parameters for secondary weights
        self.h = torch.zeros(self.actions, self.features, requires_grad=False).to(cfg.device)
        # ADAM optimizer parameters for secondary weights
        self.v = torch.zeros(self.actions, self.features, requires_grad=False).to(cfg.device)
        self.m = torch.zeros(self.actions, self.features, requires_grad=False).to(cfg.device)

        if 'load_params' in self.cfg.secn_weight_config and self.cfg.secn_weight_config['load_params']:
            self.load_2nd_weight_param(cfg.secn_weight_config['path'])

        if cfg.agent_name == 'QRCOnline' and cfg.load_offline_data:
            self.fill_offline_data_to_buffer()

    def update(self, data):
        states, actions, rewards, next_states, terminals = data['obs'], data['act'], data['reward'], data['obs2'], data['done']
        
        x = self.rep_net(states)
        Qs = self.val_net(x)
        Qsa = Qs[self.batch_indices, actions]

        # Constructing the target
        q_next = self.targets.val_net(self.targets.rep_net(next_states))
        q_next = q_next.max(1)[0]
        target = self.cfg.discount * q_next * (1 - terminals)#.float()
        target.add_(rewards.float())
        td_loss = 0.5 * self.vf_loss(Qsa, target)
        
        # compute E[\delta | x] ~= <h, x>
        with torch.no_grad():
            delta_hats = torch.matmul(x, self.h.t())
            delta_hat = delta_hats[self.batch_indices, actions]

        correction_loss = torch.mean(self.cfg.discount * delta_hat * q_next * (1 - terminals))

        # make sure we have no gradients left over from previous update
        self.optimizer.zero_grad()
        td_loss.backward(retain_graph=True)
        self.optimizer.step()

        self.targets.rep_net.zero_grad()
        self.targets.val_net.zero_grad()
        self.optimizer.zero_grad()
        correction_loss.backward()

        # add the gradients of the target network for the correction term to the gradients for the td error
        for (policy_param, target_param) in zip(self.rep_net.parameters(), self.targets.rep_net.parameters()):
            policy_param.grad.add_(target_param.grad)
        for (policy_param, target_param) in zip(self.val_net.parameters(), self.targets.val_net.parameters()):
            policy_param.grad.add_(target_param.grad)
        # update the *policy network* using the combined gradients
        # self.targets.rep_net.zero_grad()
        # self.targets.val_net.zero_grad()
        self.optimizer.step()

        # update the secondary weights using a *fixed* feature representation generated by the policy network
        with torch.no_grad():
            delta = target - Qsa
            # dh = (delta - delta_hat).reshape((1, -1)) * x
            temp = (delta - delta_hat).reshape((1, -1)).repeat(x.size(1), 1).T
            dh = temp * x

            # compute the update for each action independently
            # assume that there is a separate `h` vector for each individual action
            for a in range(self.actions):
                # mask = (batch.actions == a).squeeze(1)
                mask = actions == a
                # if this action was never taken in this mini-batch
                # then skip the update for this action
                if mask.sum() == 0:
                    continue

                # the update for `h` minus the regularizer
                h_update = dh[mask].mean(0) - self.beta * self.h[a]

                # ADAM optimizer
                # keep a separate set of weights for each action here as well
                self.v[a] = self.beta_2 * self.v[a] + (1 - self.beta_2) * (h_update**2)
                self.m[a] = self.beta_1 * self.m[a] + (1 - self.beta_1) * h_update

                self.h[a] = self.h[a] + self.alpha * self.m[a] / (torch.sqrt(self.v[a]) + self.eps)

        if self.cfg.use_target_network and self.total_steps % self.cfg.target_network_update_freq == 0:
            self.sync_target()
            # self.targets.rep_net.load_state_dict(self.rep_net.state_dict())
            # self.targets.val_net.load_state_dict(self.val_net.state_dict())
         
    def save(self, early=False):
        parameters_dir = self.cfg.get_parameters_dir()
        if early:
            path = os.path.join(parameters_dir, "rep_net_earlystop")
        elif self.cfg.checkpoints:
            path = os.path.join(parameters_dir, "rep_net_{}".format(self.total_steps))
        else:
            path = os.path.join(parameters_dir, "rep_net")
        torch.save(self.rep_net.state_dict(), path)

        if early:
            path = os.path.join(parameters_dir, "val_net_earlystop")
        else:
            path = os.path.join(parameters_dir, "val_net")
        torch.save(self.val_net.state_dict(), path)

        if early:
            path = os.path.join(parameters_dir, "secn_weight_h_earlystop")
        else:
            path = os.path.join(parameters_dir, "secn_weight_h")
        torch.save(self.h, path)

        if early:
            path = os.path.join(parameters_dir, "secn_weight_v_earlystop")
        else:
            path = os.path.join(parameters_dir, "secn_weight_v")
        torch.save(self.v, path)

        if early:
            path = os.path.join(parameters_dir, "secn_weight_m_earlystop")
        else:
            path = os.path.join(parameters_dir, "secn_weight_m")
        torch.save(self.m, path)

    def load_2nd_weight_param(self, parameters_dir):
        path = os.path.join(self.cfg.data_root, parameters_dir["h"])
        self.h = torch.load(path, map_location=self.cfg.device)
        path = os.path.join(self.cfg.data_root, parameters_dir["v"])
        self.v = torch.load(path, map_location=self.cfg.device)
        path = os.path.join(self.cfg.data_root, parameters_dir["m"])
        self.m = torch.load(path, map_location=self.cfg.device)
        self.cfg.logger.info("Load 2nd weight parameter from {}".format(path))


class QRCOffline(QRCOnline):
    def __init__(self, cfg):
        super(QRCOffline, self).__init__(cfg)
        self.offline_param_init()
        
    def get_data(self):
        return self.get_offline_data()
        
    def feed_data(self):
        self.update_stats(0, None)
        return

